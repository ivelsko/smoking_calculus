# Smokers calculus (Middenbeemster calculus) Notes

################################################################

# eager2 to process data


# use the KappaHiFiUracil+ data b/c the enzyme is a proofreading one and it should show damage patterns
# also it seems to be slightly deeper sequenced than the PFU turbo library sequence data
# run fastQC on the files here to see if they still have internal index sequences that need to be removed: 
# /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/ENAupload

#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-99%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/ENAupload/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/ENAupload/FastQC

# symlink the files to a new folder and give them in-house-style names
/projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/inhouseformat

# ran AdapterRemoval --identify-adapters on several and it didn't pick up the internal index, so run them through 
# eager2 and then check post AR fastqc to see if they have an internal barcode that still needs to be removed



nextflow run nf-core/eager \
-r dev 588e470e2a \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/eager2 \
-work-dir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/work \
--input /projects1/microbiome_calculus/strepSABP/00-documentation.backup/Radcliffe_kHFu_eager2.tsv \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--email velsko@shh.mpg.de \
-name radcliffe_kHFu \
-with-tower




# from the fastqc after adapter removal, we can see there is still a 6bp internal barcode on each read
# remove this internal barcode with cutadapt

#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-99%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter -name '*.gz' -type f | rev | cut -d/ -f1 | rev) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

cutadapt -u 6 -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/internal_index_removed/$(basename "${SAMPLENAME}" .unmapped.fastq.gz).unmapped.clip.fastq.gz /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter/"${SAMPLENAME}"


# now check them with fastqc to make sure the 6 bases are gone
#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-49%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/internal_index_removed/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/internal_index_removed/FastQC


rename 's/.unmapped.clip.fastq.gz/_unmapped.clip.fastq.gz/' *.unmapped.clip.fastq.gz

#####
# repeat with the PFUTurbo libraries
nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/eager2 \
-work-dir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/work \
--input /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/Radcliffe_PFUturbo_eager2_map_human.tsv \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--email velsko@shh.mpg.de \
-name radcliffe_pfuT \
-with-tower

# Check the fastQC files from eager after adapter removal to see if they have this 6bp internal index still attached

#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-99%4
#SBATCH -J "rad_cut6"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter -name '*.gz' -type f | rev | cut -d/ -f1 | rev) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

cutadapt -u 6 -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/internal_index_removed/$(basename "${SAMPLENAME}" .unmapped.fastq.gz).unmapped.clip.fastq.gz /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter/"${SAMPLENAME}"


# now check them with fastqc to make sure the 6 bases are gone
#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-49%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/internal_index_removed/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/internal_index_removed/FastQC


rename 's/.unmapped.clip.fastq.gz/_unmapped.clip.fastq.gz/' *.unmapped.clip.fastq.gz

#!/bin/bash

#SBATCH -n 4
#SBATCH --mem 4G
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.mapdmg.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.mapdmg.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH -J "mpdmg"


for f in CS3*; do

bwa aln -n 0.02 -l 1024 /projects1/microbiome_sciences/reference_genomes/Tannerella_forsythia/Tannerella_forsythia_9212.fa $f > ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sai

bwa samse /projects1/microbiome_sciences/reference_genomes/Tannerella_forsythia/Tannerella_forsythia_9212.fa ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sai $f > ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sam

samtools view -bS ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sam | samtools sort - ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf

samtools rmdup -s ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.bam ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.rmdup.bam

/projects1/tools/mapdamage_2.0.6/mapDamage -i ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.rmdup.bam -r /projects1/microbiome_sciences/reference_genomes/Tannerella_forsythia

rm ./mapdamage/*.sam

done

# and finally run eager2 again to profile the samples to compare with the KappHiFiU+ profiles
# if they are similar, combine the libraries with another eager2 run and make a new MALT profile

nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe_pfuT/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe_pfuT/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/radcliffe_pfuT_eager2_input.tsv \
--fasta /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/Coliphage_phiX-174.fa \
--bwa_index /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--malt_sam_output \
--email velsko@shh.mpg.de \
-name radcliffe_pfuT_tsv \
-with-tower


##############
# For Lena's smokers calc project

# Kilteasheen 
nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/kilteasheen/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/kilteasheen/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/kilteasheen_eager2_input.tsv \
--complexity_filter_poly_g \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--malt_sam_output \
--email velsko@shh.mpg.de \
-name kilteasheen_tsv \
-with-tower

# Radcliffe KappaHiFiUracil+
nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/radcliffe_eager2_input.tsv \
--fasta /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/Coliphage_phiX-174.fa \
# --skip_fastqc \
# --skip_adapterremoval \
--bwa_index /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--malt_sam_output \
--email velsko@shh.mpg.de \
-name radcliffe_tsv \
-with-tower


################################################################
# SourceTracker

# symlink the collapsed, quality-filterd reads for sourceTracker
cd /projects1/microbiome_calculus/smoking_calculus/04-anaysis/sourceTracker/input
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/samtools/filter/*.gz
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/ELR*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/IVE*.gz .

# remove the extraction and library control blanks

# Don't bother with 16S mapping, just use shotgun data
# For shotgun data, take the sources from the DeepEvo table James made
# Take the metadata sheet from Zandra's modern calculus SourceTracker run
# Combine DeepEvo sources and the sinks from this project in R
# add the samples from this project to the metadata table
# convert the species header to match QIIME v1 formatting
# add '# Constructed from biom file' as the 1st line and
# replace Species with '#OTU ID'
# remove quotes around NA in metadata file perl -p -i -e 's/\"NA\"/NA/g' source_tracker_mappingfile_new.tsv

################################################################
# GC content and read length estimates

# work here /projects1/microbiome_calculus/smoking_calculus/04-analysis/GC_RL
# test file abpOther_Group4_extraction.fasta
# this works on symlinked files

ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/ELR*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/IVE*.gz .


infoseq -auto -outfile gc_rl_test_out_gz.tsv -only -name -length -pgc abpOther_Group4_extraction.fasta.gz
infoseq -auto -outfile gc_rl_LIB_test_out_gz.tsv -only -name -length -pgc ./input/LIB058.A0102.unmapped.fastq

snakemake -s ../../02-scripts.backup/gc_rl_calc.Snakefile --cluster 'sbatch --mem 4G -p long -n 4' -j 1 -k --restart-times 3 -n

# Alternately get the lane averages from multiqc here:
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/multiqc/multiqc_data/multiqc_fastqc_1.txt
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/multiqc/radcliffe_pfuT_tsv_multiqc_report_data/multiqc_general_stats.txt
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/multiqc/multiqc_data/multiqc_general_stats.txt
/projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/multiqc/multiqc_data/multiqc_fastqc_1.txt



################################################################
# Now we need to run several more cycles of MALT, to test differences in sequencing depth, read length, and paired-end vs single-end sequencing
# Only the single-end test requires a full eager2 run
# Make a single input tsv file that has all R1 files of all the sample groups
# This can be run last, after looking at the sequencing depth and read length changes

# All single-end 
nextflow run nf-core/eager \
-r 2.3.1 \
-profile sdag,ssh,microbiome_screening \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/single-end/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/single-end/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/single-end_eager2_input.tsv \
--complexity_filter_poly_g \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--email velsko@shh.mpg.de \
-name single-end_tsv \
-with-tower

# get radcliffe info from here--input /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/Radcliffe_PFUturbo_eager2_map_human.tsv \
# need to process the uncollapsed reads as SE - 2 eager2 runs, 1 mapping to the phiX genome, then the 2nd with all samples mapping to the human genome
# need to get the iberian info from the iberian data folder
# also add JAE!!

# For the sequencing depth and read length normalization:

# To subsample to same depth use seqtk
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/samtools/filter/*.gz
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/samtools/filter/*.gz
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/*.gz
/projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/JAE*.gz
/projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/ELR*.gz
/projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/IVE*.gz
/projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/EXB*.gz
/projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/LIB*.gz

for f in /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/checks_input/*.gz
do
seqtk sample -s10000 $f 10000000 > /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/checks_input/sub10M/$(basename $f .gz).10M.gz
done


# To subsample to have only reads with length <76bp use bioawk
for f in /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/full_files/*.gz
do
bioawk -c fastx '{if (length($seq) < 76){print "@"$name" "$comment"\n"$seq"\n+\n"$qual}}' $f > $(basename $f .gz).75bp.gz
done

################################################################

# Check the RL and GC content of the mapped vs unmapped reads
# use scripts for bones/KRA (uses samtools grep RL and GCD)

# for MALT-mapped sam files
#!/bin/bash

#SBATCH -n 2
#SBATCH --mem 2G
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --export=ALL
#SBATCH --array=0-95%8
#SBATCH -J "mappedRL"

SAMPLES=($(find /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/* -name '*.sam.gz' -type f | rev | cut -d/ -f 1 | rev | cut -d. -f 1,2 | sort | uniq))
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

samtools stats /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/"$SAMPLENAME".unmapped.blastn.sam.gz | grep ^RL | cut -f 2- > /projects1/microbiome_calculus/smoking_calculus/04-analysis/RL_GC/full_files/"$SAMPLENAME".malt_mapped_readlength.tsv
samtools stats /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/"$SAMPLENAME".unmapped.blastn.sam.gz | grep ^GCF | cut -f 2- > /projects1/microbiome_calculus/smoking_calculus/04-analysis/RL_GC/full_files/"$SAMPLENAME".malt_mapped_gc.tsv

# for MALT-unmapped fasta files:

# First get the unmapped reads from the MALT bacteria/archaea/homo output:
# 1. get a list of all *mapped* reads from the sam.gz output file
# 2. Use setkit to pull reads NOT on that list from the original fastq files into new fasta files

#!/bin/bash

#SBATCH -n 4
#SBATCH --mem 4G
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.unmapped.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.unmapped.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH -J "unmapped"

for f in /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.sam.gz
do
zcat $f | grep -v "^@" | awk -F"\t" '{print $1}' | sort | uniq > /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/$(basename $f .unmapped.blastn.sam.gz).malt_mapped.tsv
zcat /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/$(basename $f .unmapped.blastn.sam.gz).unmapped.fastq.gz | seqkit fq2fa | seqkit grep -v -f /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/$(basename $f .unmapped.blastn.sam.gz).malt_mapped.tsv > /projects1/microbiome_misc/smoking_calculus/03-preprocessing/bones/malt_unmapped_fasta/$(basename $f .unmapped.blastn.sam.gz)_MALT_unmapped.fasta
pigz -p 4 /projects1/microbiome_misc/smoking_calculus/03-preprocessing/bones/malt_unmapped_fasta/$(basename $f .unmapped.blastn.sam.gz)_MALT_unmapped.fasta
done

# Run snakefile to get these
# Make another file for 10M and 75bp, when their MALT runs are done
snakemake -s ../../../../../02-scripts.backup/gc_rl_calc_malt_mapped.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 8 -k --restart-times 3 -n


# then run multiqc in the fastqc folder to get avg RL and GC content of each sample
/projects1/microbiome_calculus/smoking_calculus/04-analysis/gc_rl_sub_mapped/output/FastQC multiqc .


# now calculate the avg read length and GC content with FastQC

# Or use EMBOSS infoseq to make tsv files?
# Can these be read into R quickly on the SHH network (not at home)?
# adjust this file
# need to convert sam files to fasta
snakemake -s ../../02-scripts.backup/gc_rl_calc.Snakefile --cluster 'sbatch --mem 4G -p long -n 4' -j 1 -k --restart-times 3 -n

# also get the new sequencing depth of sub10M files and sub75bp files


rename 's/.fastq.10M.gz/.10M.fastq/' *.fastq.10M.gz
rename 's/.fastq.75bp.gz/.75bp.fastq/' *.fastq.75bp.gz
gzip *.gz

# Use FastQC to get average read length and average GC content of 10M and 75bp sub-sampled fastq files
#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-47%4
#SBATCH -J "10M_fastqc"

SAMPLES=( $(find /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/sub75bp/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/sub75bp/FastQC

# run multiqc in both output folders
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/sub75bp/FastQC multiqc .
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/depth_check/sub10M/FastQC multiqc .

# for the 10M folders, it only looks at the full files, not the sym-linked originals
# take the avg read length and avg GC content from the original eager2 run for these

# Run snakefile to get these
# Make another file for 10M and 75bp, when their MALT runs are done
snakemake -s ../../../../02-scripts.backup/gc_rl_calc_sub10M_mapped.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 8 -k --restart-times 3 -n
snakemake -s ../../../../02-scripts.backup/gc_rl_calc_sub75bp_mapped.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 8 -k --restart-times 3 -n


################################################################

# HHV4/5 investigation

# calculate the ANI of all genomes - are any below 95%?
/projects1/microbiome_calculus/smoking_calculus/01-data/hhv4/hhv4_ncbi-genomes-2021-03-02
gunzip *.gz
ls /projects1/microbiome_calculus/smoking_calculus/01-data/hhv4/hhv4_ncbi-genomes-2021-03-02/*fna > hhv4_query_list.tsv
ls /projects1/microbiome_calculus/smoking_calculus/01-data/hhv4/hhv4_ncbi-genomes-2021-03-02/*fna > hhv4_reference_list.tsv
fastANI --ql hhv4_query_list.tsv --rl hhv4_reference_list.tsv --fragLen 1000 -o fastani_hhv4_out.tsv &> fastani.log

# also cat all genomes together for alignment with DECIPHER
cat *.fna > all_hhv4_genomes.fasta

# use DECIPHER to align and cluster the HHV4 genomes to decide how many to use for mapping

# it runs too slow on my computer to use MID_hhv4_ani
# use instead the snakemake file
snakemake -s ../../02-scripts.backup/hhv4_decipher.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 1 -k --restart-times 3 -n

# bwa index 1 of the 2 complete genomes (need to re-do with a RefSeq assembly instead of a GenBank assembly)

snakemake -s ../../../02-scripts.backup/007a-bwamap_hhv4.Snakefile --cluster 'sbatch --mem 12G -p short -n {threads}' -j 8 -k --restart-times 3 -n
snakemake -s ../../../02-scripts.backup/007b-bwamap_hhv5.Snakefile --cluster 'sbatch --mem 12G -p short -n {threads}' -j 8 -k --restart-times 3 -n




rm FastQC/MID084.A0101* FastQC/KT19calc* FastQC/MID092.A0101* FastQC/MID068.A0101* FastQC/KT31calc*


















