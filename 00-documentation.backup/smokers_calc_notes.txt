# Smokers calculus (Middenbeemster calculus) Notes

################################################################

# eager2 to process data
#Lena Semerau, 01.07.2020

nextflow run nf-core/eager \
-profile shh,sdag \
-r dev \
--paired-end \
--input '/projects1/microbiome_calculus/smokers/00-documentation.backup/eager2_smokers_input_table_new.tsv' \
--fasta /projects1/Reference_Genomes/Human/hs37d5/hs37d5.fa \
--bam_index /projects1/Reference_Genomes/Human/hs37d5/hs37d5.fa \
--seq_dict /projects1/Reference_Genomes/Human/hs37d5/hs37d5.dict \
--fasta_index /projects1/Reference_Genomes/Human/hs37d5/hs37d5.fa.fai \
--skip_preseq \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/malt/databases/raw/refseq-bac-arch-homo-2018_11/ \
-name smokers_calculus \
--outdir /projects1/microbiome_calculus/smokers/03-preprocessing/ \
-work-dir /projects1/microbiome_calculus/smokers/03-preprocessing/work/

# Radcliffe data
# use the KappaHiFiUracil+ data b/c the enzyme is a proofreading one and it should show damage patterns
# also it seems to be slightly deeper sequenced than the PFU turbo library sequence data
# run fastQC on the files here to see if they still have internal index sequences that need to be removed: 
# /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/ENAupload

#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-99%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/ENAupload/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/ENAupload/FastQC

# symlink the files to a new folder and give them in-house-style names
/projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/raw_reads/inhouseformat

# ran AdapterRemoval --identify-adapters on several and it didn't pick up the internal index, so run them through 
# eager2 and then check post AR fastqc to see if they have an internal barcode that still needs to be removed



nextflow run nf-core/eager \
-r dev 588e470e2a \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/eager2 \
-work-dir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/work \
--input /projects1/microbiome_calculus/strepSABP/00-documentation.backup/Radcliffe_kHFu_eager2.tsv \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--email velsko@shh.mpg.de \
-name radcliffe_kHFu \
-with-tower




# from the fastqc after adapter removal, we can see there is still a 6bp internal barcode on each read
# remove this internal barcode with cutadapt

#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-99%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter -name '*.gz' -type f | rev | cut -d/ -f1 | rev) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

cutadapt -u 6 -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/internal_index_removed/$(basename "${SAMPLENAME}" .unmapped.fastq.gz).unmapped.clip.fastq.gz /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter/"${SAMPLENAME}"


# now check them with fastqc to make sure the 6 bases are gone
#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-49%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/internal_index_removed/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/kapaHiFiU_libraries/adapter_removed/MPI-SHH/internal_index_removed/FastQC


rename 's/.unmapped.clip.fastq.gz/_unmapped.clip.fastq.gz/' *.unmapped.clip.fastq.gz

#####
# repeat with the PFUTurbo libraries
nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/eager2 \
-work-dir /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/work \
--input /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/Radcliffe_PFUturbo_eager2_map_human.tsv \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--email velsko@shh.mpg.de \
-name radcliffe_pfuT \
-with-tower

# Check the fastQC files from eager after adapter removal to see if they have this 6bp internal index still attached

#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-99%4
#SBATCH -J "rad_cut6"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter -name '*.gz' -type f | rev | cut -d/ -f1 | rev) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

cutadapt -u 6 -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/internal_index_removed/$(basename "${SAMPLENAME}" .unmapped.fastq.gz).unmapped.clip.fastq.gz /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/eager2/samtools/filter/"${SAMPLENAME}"


# now check them with fastqc to make sure the 6 bases are gone
#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-49%4
#SBATCH -J "rad_fastqc"

SAMPLES=( $(find /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/internal_index_removed/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/adapter_removed/MPI-SHH/internal_index_removed/FastQC


rename 's/.unmapped.clip.fastq.gz/_unmapped.clip.fastq.gz/' *.unmapped.clip.fastq.gz

#!/bin/bash

#SBATCH -n 4
#SBATCH --mem 4G
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.mapdmg.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.mapdmg.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH -J "mpdmg"


for f in CS3*; do

bwa aln -n 0.02 -l 1024 /projects1/microbiome_sciences/reference_genomes/Tannerella_forsythia/Tannerella_forsythia_9212.fa $f > ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sai

bwa samse /projects1/microbiome_sciences/reference_genomes/Tannerella_forsythia/Tannerella_forsythia_9212.fa ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sai $f > ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sam

samtools view -bS ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.sam | samtools sort - ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf

samtools rmdup -s ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.bam ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.rmdup.bam

/projects1/tools/mapdamage_2.0.6/mapDamage -i ./mapdamage/$(basename $f unmapped.clip.fastq.gz)Tf.rmdup.bam -r /projects1/microbiome_sciences/reference_genomes/Tannerella_forsythia

rm ./mapdamage/*.sam

done

# and finally run eager2 again to profile the samples to compare with the KappHiFiU+ profiles
# if they are similar, combine the libraries with another eager2 run and make a new MALT profile

nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe_pfuT/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe_pfuT/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/radcliffe_pfuT_eager2_input.tsv \
--fasta /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/Coliphage_phiX-174.fa \
--bwa_index /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--malt_sam_output \
--email velsko@shh.mpg.de \
-name radcliffe_pfuT_tsv \
-with-tower


##############
# For Lena's smokers calc project

# Kilteasheen 
nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/kilteasheen/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/kilteasheen/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/kilteasheen_eager2_input.tsv \
--complexity_filter_poly_g \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--malt_sam_output \
--email velsko@shh.mpg.de \
-name kilteasheen_tsv \
-with-tower

# Radcliffe KappaHiFiUracil+
nextflow run nf-core/eager \
-r 2.2.1 \
-profile microbiome_screening,shh \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/radcliffe/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/radcliffe_eager2_input.tsv \
--fasta /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/Coliphage_phiX-174.fa \
# --skip_fastqc \
# --skip_adapterremoval \
--bwa_index /projects1/microbiome_sciences/reference_genomes/Coliphage_phiX-174/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--malt_sam_output \
--email velsko@shh.mpg.de \
-name radcliffe_tsv \
-with-tower


################################################################
# SourceTracker

# symlink the collapsed, quality-filterd reads for sourceTracker
cd /projects1/microbiome_calculus/smoking_calculus/04-anaysis/sourceTracker/input
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/samtools/filter/*.gz
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/ELR*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/IVE*.gz .

# remove the extraction and library control blanks

# Don't bother with 16S mapping, just use shotgun data
# For shotgun data, take the sources from the DeepEvo table James made
# Take the metadata sheet from Zandra's modern calculus SourceTracker run
# Combine DeepEvo sources and the sinks from this project in R
# add the samples from this project to the metadata table
# convert the species header to match QIIME v1 formatting
# add '# Constructed from biom file' as the 1st line and
# replace Species with '#OTU ID'
# remove quotes around NA in metadata file perl -p -i -e 's/\"NA\"/NA/g' source_tracker_mappingfile_new.tsv

################################################################
# GC content and read length estimates

# work here /projects1/microbiome_calculus/smoking_calculus/04-analysis/GC_RL
# test file abpOther_Group4_extraction.fasta
# this works on symlinked files

ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/samtools/filter/*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/ELR*.gz .
ln -s /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/IVE*.gz .


infoseq -auto -outfile gc_rl_test_out_gz.tsv -only -name -length -pgc abpOther_Group4_extraction.fasta.gz
infoseq -auto -outfile gc_rl_LIB_test_out_gz.tsv -only -name -length -pgc ./input/LIB058.A0102.unmapped.fastq

snakemake -s ../../02-scripts.backup/gc_rl_calc.Snakefile --cluster 'sbatch --mem 4G -p long -n 4' -j 1 -k --restart-times 3 -n

# Alternately get the lane averages from multiqc here:
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/multiqc/multiqc_data/multiqc_fastqc_1.txt
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/multiqc/radcliffe_pfuT_tsv_multiqc_report_data/multiqc_general_stats.txt
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/multiqc/multiqc_data/multiqc_general_stats.txt
/projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/multiqc/multiqc_data/multiqc_fastqc_1.txt



################################################################
# Now we need to run several more cycles of MALT, to test differences in sequencing depth, read length, and paired-end vs single-end sequencing
# Only the single-end test requires a full eager2 run
# Make a single input tsv file that has all R1 files of all the sample groups
# This can be run last, after looking at the sequencing depth and read length changes

# All single-end 
nextflow run nf-core/eager \
-r 2.3.1 \
-profile sdag,ssh,microbiome_screening \
--outdir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/single-end/eager2 \
-work-dir /projects1/microbiome_calculus/smokers_calculus/03-preprocessing/single-end/work \
--input /projects1/microbiome_calculus/smokers_calculus/00-documentation.backup/single-end_eager2_input.tsv \
--complexity_filter_poly_g \
--fasta /projects1/Reference_Genomes/Human/HG19/hg19_complete.fasta \
--bwa_index /projects1/Reference_Genomes/Human/HG19/ \
--bwaalnn 0.02 \
--bwaalnl 1024 \
--run_bam_filtering \
--bam_discard_unmapped \
--bam_unmapped_type fastq \
--skip_damage_calculation \
--skip_qualimap \
--run_metagenomic_screening \
--metagenomic_tool malt \
--database /projects1/microbiome_sciences/reference_databases/built/refseq/bacteria_archea_homo_20181122/malt/ \
--email velsko@shh.mpg.de \
-name single-end_tsv \
-with-tower

# get radcliffe info from here--input /projects1/microbiome_sciences/raw_data/external.backup/PaleoBARN/pfu_turbo_libraries/Radcliffe_PFUturbo_eager2_map_human.tsv \
# need to process the uncollapsed reads as SE - 2 eager2 runs, 1 mapping to the phiX genome, then the 2nd with all samples mapping to the human genome
# need to get the iberian info from the iberian data folder
# also add JAE!!

# For the sequencing depth and read length normalization:

# To subsample to same depth use seqtk
/mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/kilteasheen/eager2/samtools/filter/*.gz
/mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/radcliffe_pfuT/eager2/samtools/filter/*.gz
/mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/*.gz
/mnt/archgen/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/JAE*.gz
/mnt/archgen/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/ELR*.gz
/mnt/archgen/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/IVE*.gz
/mnt/archgen/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/EXB*.gz
/mnt/archgen/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/LIB*.gz

for f in /mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/checks_input/*.gz
do
seqtk sample -s10000 $f 10000000 > /mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/checks_input/sub10M/$(basename $f .fastq.gz).10M.fastq.gz
done

for f in /projects1/microbiome_calculus/iberian/03-preprocessing/screening/eager2/samtools/filter/VLC*.gz
do
seqtk sample -s10000 $f 10000000 > /mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/depth_check/sub10M/$(basename $f .fastq.gz).10M.fastq.gz
done


# To subsample to have only reads with length <76bp use bioawk
for f in /mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/full_files/*.gz
do
bioawk -c fastx '{if (length($seq) < 76){print "@"$name" "$comment"\n"$seq"\n+\n"$qual}}' $f > $(basename $f .gz).75bp.gz
done


################################################################

# Check the RL and GC content of the mapped vs unmapped reads
# use scripts for bones/KRA (uses samtools grep RL and GCD)

# for MALT-mapped sam files
#!/bin/bash

#SBATCH -n 2
#SBATCH --mem 2G
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --export=ALL
#SBATCH --array=0-95%8
#SBATCH -J "mappedRL"

SAMPLES=($(find /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/* -name '*.sam.gz' -type f | rev | cut -d/ -f 1 | rev | cut -d. -f 1,2 | sort | uniq))
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

samtools stats /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/"$SAMPLENAME".unmapped.blastn.sam.gz | grep ^RL | cut -f 2- > /projects1/microbiome_calculus/smoking_calculus/04-analysis/RL_GC/full_files/"$SAMPLENAME".malt_mapped_readlength.tsv
samtools stats /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/"$SAMPLENAME".unmapped.blastn.sam.gz | grep ^GCF | cut -f 2- > /projects1/microbiome_calculus/smoking_calculus/04-analysis/RL_GC/full_files/"$SAMPLENAME".malt_mapped_gc.tsv

# for MALT-unmapped fasta files:

# First get the unmapped reads from the MALT bacteria/archaea/homo output:
# 1. get a list of all *mapped* reads from the sam.gz output file
# 2. Use setkit to pull reads NOT on that list from the original fastq files into new fasta files

#!/bin/bash

#SBATCH -n 4
#SBATCH --mem 4G
#SBATCH --partition=long
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.unmapped.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.unmapped.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH -J "unmapped"

for f in /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.sam.gz
do
zcat $f | grep -v "^@" | awk -F"\t" '{print $1}' | sort | uniq > /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/$(basename $f .unmapped.blastn.sam.gz).malt_mapped.tsv
zcat /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/samtools/filter/$(basename $f .unmapped.blastn.sam.gz).unmapped.fastq.gz | seqkit fq2fa | seqkit grep -v -f /projects1/microbiome_misc/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/$(basename $f .unmapped.blastn.sam.gz).malt_mapped.tsv > /projects1/microbiome_misc/smoking_calculus/03-preprocessing/bones/malt_unmapped_fasta/$(basename $f .unmapped.blastn.sam.gz)_MALT_unmapped.fasta
pigz -p 4 /projects1/microbiome_misc/smoking_calculus/03-preprocessing/bones/malt_unmapped_fasta/$(basename $f .unmapped.blastn.sam.gz)_MALT_unmapped.fasta
done

# Run snakefile to get these
# Make another file for 10M and 75bp, when their MALT runs are done
snakemake -s ../../../../../02-scripts.backup/gc_rl_calc_malt_mapped.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 8 -k --restart-times 3 -n


# then run multiqc in the fastqc folder to get avg RL and GC content of each sample
/projects1/microbiome_calculus/smoking_calculus/04-analysis/gc_rl_sub_mapped/output/FastQC multiqc .


# now calculate the avg read length and GC content with FastQC

# Or use EMBOSS infoseq to make tsv files?
# Can these be read into R quickly on the SHH network (not at home)?
# adjust this file
# need to convert sam files to fasta
snakemake -s ../../02-scripts.backup/gc_rl_calc.Snakefile --cluster 'sbatch --mem 4G -p long -n 4' -j 1 -k --restart-times 3 -n

# also get the new sequencing depth of sub10M files and sub75bp files


rename 's/.fastq.10M.gz/.10M.fastq/' *.fastq.10M.gz
rename 's/.fastq.75bp.gz/.75bp.fastq/' *.fastq.75bp.gz
gzip *.gz

# Use FastQC to get average read length and average GC content of 10M and 75bp sub-sampled fastq files
#!/usr/bin/env bash

#SBATCH -c 4
#SBATCH --mem=32000
#SBATCH --partition=short
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --array=0-47%4
#SBATCH -J "10M_fastqc"

SAMPLES=( $(find /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/sub75bp/ -name '*.gz' -type f) )
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

fastqc "${SAMPLENAME}" -o /projects1/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/sub75bp/FastQC

# run multiqc in both output folders
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/sub75bp/FastQC multiqc .
/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/depth_check/sub10M/FastQC multiqc .

# for the 10M folders, it only looks at the full files, not the sym-linked originals
# take the avg read length and avg GC content from the original eager2 run for these

# Run snakefile to get these
# Make another file for 10M and 75bp, when their MALT runs are done
snakemake -s ../../../../02-scripts.backup/gc_rl_calc_sub10M_mapped.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 8 -k --restart-times 3 -n
snakemake -s ../../../../02-scripts.backup/gc_rl_calc_sub75bp_mapped.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 8 -k --restart-times 3 -n


# get the average GC content and average RL of the VLC sub10M/sub75bp samples
# use FASTQC
snakemake -s ../../../../02-scripts.backup/sub10M_fastqc_gc_rl_calc.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 16 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 10 --latency-wait 20 -n
snakemake -s ../../../../02-scripts.backup/sub75bp_fastqc_gc_rl_calc.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 16 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 10 --latency-wait 20 -n

# then run multiqc in the fastqc folder to get avg RL and GC content of each sample
# run here: /mnt/archgen/mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/depth_check/sub10M/FastQC 
# and here: /mnt/archgen/mnt/archgen/microbiome_calculus/smoking_calculus/03-preprocessing/read_len_check/sub75bp/FastQC 
multiqc .


################################################################

# HHV4/5 investigation

# calculate the ANI of all genomes - are any below 95%?
/projects1/microbiome_calculus/smoking_calculus/01-data/hhv4/hhv4_ncbi-genomes-2021-03-02
gunzip *.gz
ls /projects1/microbiome_calculus/smoking_calculus/01-data/hhv4/hhv4_ncbi-genomes-2021-03-02/*fna > hhv4_query_list.tsv
ls /projects1/microbiome_calculus/smoking_calculus/01-data/hhv4/hhv4_ncbi-genomes-2021-03-02/*fna > hhv4_reference_list.tsv
fastANI --ql hhv4_query_list.tsv --rl hhv4_reference_list.tsv --fragLen 1000 -o fastani_hhv4_out.tsv &> fastani.log

# also cat all genomes together for alignment with DECIPHER
cat *.fna > all_hhv4_genomes.fasta

# use DECIPHER to align and cluster the HHV4 genomes to decide how many to use for mapping

# it runs too slow on my computer to use MID_hhv4_ani
# use instead the snakemake file
snakemake -s ../../02-scripts.backup/hhv4_decipher.Snakefile --cluster 'sbatch --mem 8G -p short -n {threads}' -j 1 -k --restart-times 3 -n

# bwa index 1 of the 2 complete genomes (need to re-do with a RefSeq assembly instead of a GenBank assembly)

snakemake -s ../../../02-scripts.backup/007a-bwamap_hhv4.Snakefile --cluster 'sbatch --mem 12G -p short -n {threads}' -j 8 -k --restart-times 3 -n
snakemake -s ../../../02-scripts.backup/007b-bwamap_hhv5.Snakefile --cluster 'sbatch --mem 12G -p short -n {threads}' -j 8 -k --restart-times 3 -n




rm FastQC/MID084.A0101* FastQC/KT19calc* FastQC/MID092.A0101* FastQC/MID068.A0101* FastQC/KT31calc*


################################################################
# HOPS/MALT extract to look at alignments against odd taxa driving horseshoe shape
# on daghead1
conda activate /home/alexander_huebner/miniconda3/envs/hops

cd /mnt/archgen/users/velsko/smokers_calc/hops_me

# the snakemake file didn't work, but running it with the command below did
snakemake -s ./008-hops_malt_extract.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 4 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 1 --restart-times 1 --latency-wait 20 -n


/home/irina_marie_velsko/bin/miniconda2/bin/hops -Xmx200G \
    -input /mnt/archgen/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.rma6 \
    -output /mnt/archgen/users/velsko/smokers_calc/hops_me/test_out_multi \
    -m maltex \
    -c maltExtract.config


################################################################

/Users/velsko/Applications/MEGAN/tools/read-extractor -i ~/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.rma6 -o ~/projects1/microbiome_calculus/smoking_calculus/04-analysis/malt_checks/odd_taxa_gc -c Taxonomy -n 'Thermanaerovibrio_acidaminovorans' -v # 81462
/Users/velsko/Applications/MEGAN/tools/read-extractor -i ~/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.rma6 -o ~/projects1/microbiome_calculus/smoking_calculus/04-analysis/malt_checks/odd_taxa_gc -c Taxonomy -n 'Jonquetella_anthropi' -v # 428712
/Users/velsko/Applications/MEGAN/tools/read-extractor -i ~/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.rma6 -o ~/projects1/microbiome_calculus/smoking_calculus/04-analysis/malt_checks/odd_taxa_gc -c Taxonomy -n 'Streptobacillus_moniliformis' -v # 34105
/Users/velsko/Applications/MEGAN/tools/read-extractor -i ~/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.rma6 -o ~/projects1/microbiome_calculus/smoking_calculus/04-analysis/malt_checks/odd_taxa_gc -c Taxonomy -n 'Emergencia_timonensis' -v # 1776384
/Users/velsko/Applications/MEGAN/tools/read-extractor -i ~/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.rma6 -o ~/projects1/microbiome_calculus/smoking_calculus/04-analysis/malt_checks/odd_taxa_gc -c Taxonomy -n 'Anaerovorax_odorimutans_DSM_5092' -v # 109327 
/Users/velsko/Applications/MEGAN/tools/read-extractor -i ~/projects1/microbiome_calculus/smoking_calculus/03-preprocessing/eager2/eager2_run/metagenomic_classification/malt/*.rma6 -o ~/projects1/microbiome_calculus/smoking_calculus/04-analysis/malt_checks/odd_taxa_gc -c Taxonomy -n 'Shuttleworthia_satelles_DSM_14600' -v # 177972 none found


# calculate GC content with EMBOSS infoseq for each file
infoseq -auto -outfile gc_rl_test_out_gz.tsv -only -name -length -pgc abpOther_Group4_extraction.fasta.gz

for f in *.txt; do infoseq -auto -outfile $(basename $f .txt).gc_rl.tsv -only -name -length -pgc $f; done

# also calculate the average GC content and read length of palaeofeces vs modern feces, and the FTA cards that Lena analyzed (different treatments affect GC content?)

# palaeofeces
/mnt/archgen/microbiome_coprolite/coprolite_evolution/03-data/ena_submission
# modern feces
/mnt/archgen/microbiome_coprolite/coprolite_evolution/03-data/publishedData
# published palaeofeces
/mnt/archgen/microbiome_coprolite/coprolite_evolution/03-data/published_paleofeces
# FTA cards
/mnt/archgen/projects1/users/semerau/ftacards/04_analysis/eager2/samtools/filter/


bioawk -c fastx '{total_gc += gc($seq)}END{print total_gc / NR}' <fastq_file> > <fastq_file>.gz

for f in /mnt/archgen/microbiome_coprolite/coprolite_evolution/03-data/ena_submission/*.gz; do
bioawk -c fastx '{total_gc += gc($seq)}END{print total_gc / NR}' $f > $(basename $f .fastq.gz).gc.tsv
done

for f in /mnt/archgen/microbiome_coprolite/coprolite_evolution/03-data/publishedData/*.gz; do
bioawk -c fastx '{total_gc += gc($seq)}END{print total_gc / NR}' $f > $(basename $f .fastq.gz).gc.tsv
done

for f in /mnt/archgen/microbiome_coprolite/coprolite_evolution/03-data/published_paleofeces/*.gz; do
bioawk -c fastx '{total_gc += gc($seq)}END{print total_gc / NR}' $f > $(basename $f .fastq.gz).gc.tsv
done

for f in /mnt/archgen/projects1/users/semerau/ftacards/04_analysis/eager2/samtools/filter/*.gz; do
bioawk -c fastx '{total_gc += gc($seq)}END{print total_gc / NR}' $f > $(basename $f .fastq.gz).gc.tsv
done


################################################################
MetaPhlAn3 on MID and CMB

# activate the conda environment to start
conda activate mpa3

# here: /projects1/microbiome_calculus/smoking_calculus/04-analysis/metaphlan3
# snakemake -s ../../02-scripts.backup/009-metaphlan3_mid.Snakefile --cluster 'sbatch --mem 4G -p short -n {threads}' -j 4 -k --restart-times 3 -n
snakemake -s ../../02-scripts.backup/009-metaphlan3_mid.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 4 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 6 --latency-wait 20 -n

# for the sub10M and sub75bp samples
snakemake -s ../../02-scripts.backup/009-metaphlan3_mid_sub10M.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 4 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 10 --latency-wait 20 -n
snakemake -s ../../02-scripts.backup/009-metaphlan3_mid_sub75bp.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 4 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 10 --latency-wait 20 -n


# merge the tables (make sure conda environment is activated)
merge_metaphlan_tables.py full_files/*.metaphlan3.tsv > ../../05-results.backup/mpa3_abundance_table.tsv
merge_metaphlan_tables.py sub10M/*.metaphlan3.tsv > ../../05-results.backup/mpa3_abundance_table_sub10M.tsv
merge_metaphlan_tables.py sub75bp/*.metaphlan3.tsv > ../../05-results.backup/mpa3_abundance_table_sub75bp.tsv

################################################################

# Download the strep group table James made for DeepEvo and check if 
# there are "chimp-like" strep distributions in the MID samples
wget https://raw.githubusercontent.com/jfy133/Hominid_Calculus_Microbiome_Evolution/master/00-documentation.backup/25-streptococcus_cladegroup_amylasegroup_database.tsv

################################################################
# Use GRiD to see if replication status is different between different species in heavy smokers and light/non-smokers

/mnt/archgen/microbiome_calculus/smoking_calculus/04-analysis/GRiD/HOMD

snakemake -s ../../../02-scripts.backup/MID_GRiD_homd.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 4 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 1 --restart-times 1 --latency-wait 20 -n

###############################
# Get the extraction batches

library(sidora.core)
library(data.table)
library(tidyverse)
con <- get_pandora_connection("~/.credentials_sidora")
libs <- get_df("TAB_Library", con)

df_list <- get_df_list(c(
  "TAB_Sample", "TAB_Extract"
), con = con)
exbs <- join_pandora_tables(df_list)
exbs <- convert_all_ids_to_values(exbs, con)

exbs_filtered <- exbs %>% select(sample.Type_Group, extract.Full_Extract_Id, extract.Batch)  %>% filter(str_detect(extract.Full_Extract_Id, "MID"), str_detect(sample.Type_Group, "Calculus"))

exbs_trial <- exbs_filtered %>%
  mutate(extract.Batch = "Ex01_ZE_2014-02-17")
  %>%
  format_as_update_existing()

samples_updated <- samples_raw %>%
  mutate(sample.Tags = "FoodTransforms") %>%
  sidora.core::format_as_update_existing()

###############################
# For Strep group fill-in-the-unknowns, download a file of all NCBI unnamed Streptococcus assemblies at the scaffold, chromosome, or complete levels
# from here https://www.ncbi.nlm.nih.gov/genome/browse/#!/prokaryotes/13548/
# called prokaryotes.csv. Rename it to unnamed_Streptococcus.csv and move to 00-documentation.backup

# see how many in the unnamed Strep group list are in this list and get the ftp info in MID_ungrouped_Strep.Rmd

# also download at most 20 genomes of other species in other groups at the 'complete' genome level
# always take genomes w/o plasmids
# if there are >20 complete, take the first 20 w/o plasmids
# if there are <20 complete, fill up to 20 with chromosome, then scaffold
# don't use contig level unless necessary to get to 20, or there are <20 in total
# these can be found in /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/<name>Group/<species>

# rename the fasta files in the <name>Group folders so they include the species name
rename 's/genomic/genomic_australis/' *.fna
rename 's/genomic/genomic_infantis/' *.fna
rename 's/genomic/genomic_mitis/' *.fna
rename 's/genomic/genomic_oralis/' *.fna
rename 's/genomic/genomic_parasanguinis/' *.fna
rename 's/genomic/genomic_peroris/' *.fna
rename 's/genomic/genomic_pseudopneumoniae/' *.fna
rename 's/genomic/genomic_agalactiae/' *
rename 's/genomic/genomic_dysgalactiae/' *
rename 's/genomic/genomic_equi/' *
rename 's/genomic/genomic_pyogenes/' *
rename 's/genomic/genomic_uberis/' *
rename 's/genomic/genomic_downei/' *
rename 's/genomic/genomic_criceti/' *
rename 's/genomic/genomic_gallolyticus/' *
rename 's/genomic/genomic_equinus/' *
rename 's/genomic/genomic_infantarius/' *
rename 's/genomic/genomic_mutans/' *
rename 's/genomic/genomic_ratti/' *
rename 's/genomic/genomic_macacae/' *

# now 
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/sanguinisGroup/*/*.fasta > grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/sanguinisGroup/*/*.fna >> grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/mitisGroup/*/*.fna >> grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/anginosusGroup/*/*.fasta >> grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/salivariusGroup/*/*.fasta >> grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/pyogenicGroup/*/*.fasta >> grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/bovisGroup/*/*.fna >> grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/downeiGroup/*/*.fna >> grouped_Strep_genomes.tsv
ls /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/mutansGroup/*/*.fna >> grouped_Strep_genomes.tsv

# now go through grouped_Strep_genomes.tsv and select only the 1st 20 of each species (take all if <20)
# save the new file as grouped_Strep_genomes_20.tsv
cat unplaced_Strep_genomes.tsv grouped_Strep_genomes_20.tsv > Strep_genomes_for_dRep.tsv

# run dRep to cluster ungrouped Strep genomes by ANI to try to place them in the known oral strep groups
dRep compare -p 24 dRep_ANImf_out/ -g genome_path.tsv --S_algorithm ANImf -pa 0.95 -sa 0.99
dRep compare -p 24 dRep_gANI_out/ -g genome_path.tsv --S_algorithm gANI -pa 0.95 -sa 0.99

# run in snakemake here: /mnt/archgen/microbiome_calculus/abpCapture/01-data.backup/probe_design/reference_genomes/dRep
snakemake -s /mnt/archgen/microbiome_calculus/smoking_calculus/02-scripts.backup/010-dRep_Strep_nunknowns.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 24 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 1 --latency-wait 30 -n

###########################
HUMAnN3
###########################
# work here /mnt/archgen/microbiome_calculus/smoking_calculus/04-analysis/humann3/
# run humann3 with this
snakemake -s ../../02-scripts.backup/011-humann3_mid.Snakefile --cluster-config /mnt/archgen/microbiome_calculus/abpCapture/02-scripts.backup/snakemake_default.config --cluster "qsub -pe smp 16 -l virtual_free={cluster.mem},h_vmem={cluster.mem} -o {cluster.out} -e {cluster.err}" -j 10 --latency-wait 20 -n

conda activate humann3

# gene families
humann_join_tables -i output/ -o genefamilies_joined.tsv --file_name unmapped_genefamilies
humann_renorm_table --input genefamilies_joined.tsv --output genefamilies_joined_cpm.tsv --units cpm
humann_regroup_table --input genefamilies_joined_cpm.tsv --output genefamilies_joined_cpm_ur90rxn.tsv --groups uniref90_rxn
humann_rename_table --input genefamilies_joined_cpm_ur90rxn.tsv --output genefamilies_joined_cpm_ur90rxn_names.tsv -n metacyc-rxn

# pathway abundance
humann_join_tables -i output/ -o pathabundance_joined.tsv --file_name unmapped_pathabundance
humann_renorm_table --input pathabundance_joined.tsv --output pathabundance_joined_cpm.tsv --units cpm

# copy tables into the 05-results folder 
cp *.tsv ../../05-results.backup/

# To determine the # of reads that were aligned (or rather unaligned) by HUMAnN3
# working dir /mnt/archgen/microbiome_calculus/Cameroon_plaque/04-analysis/humann2/all_data_combined/input
ls *.gz > samples.list
touch samples.lines
for f in *.gz; do zcat $f | wc -l >> samples.lines; done
# add a header to each file (SampleID, Lines)
paste samples.list samples.lines > ../../../05-results.backup/sample_lines.tsv 
# check that these numbers, divided by 4, are the same as those in the metadata file column Individual_Seq_Depth_NonHuman
# if they are, use that metadata column for the R analyses, otherwise figure out why that column is wrong and fix it to match these. Use it with the correct numbers

cd ../snakemake_tmp/
touch files.names unaligned.pcts
for f in *.o; do grep "genefamilies.tsv" $f | sed 's/\/mnt\/archgen\/microbiome_calculus\/Cameroon_plaque\/04-analysis\/humann2\/all_data_combined\/output\///g' >> files.names; done
for f in *.o; do grep "Unaligned reads after translated alignment:" $f | sed 's/Unaligned\ reads\ after\ translated\ alignment\://g' | sed 's/\ \%//g' >> unaligned.pcts; done
# add a header to each file (SampleID, UnalignedPct)
paste files.names unaligned.pcts > ../../../../05-results.backup/humann2_alignment_stats.tsv



























